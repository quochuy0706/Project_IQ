{"cells":[{"cell_type":"code","source":["%pip install openai"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[3,4,5,6,7],"state":"finished","livy_statement_state":"available","session_id":"383bf70c-1243-43bf-a33e-fb9f96ddd4c5","normalized_state":"finished","queued_time":"2024-10-20T13:34:52.3049261Z","session_start_time":"2024-10-20T13:34:52.4741901Z","execution_start_time":"2024-10-20T13:35:01.816425Z","execution_finish_time":"2024-10-20T13:35:17.4907245Z","parent_msg_id":"de1dfbbf-aebd-4949-9a93-6dbc7448a421"},"text/plain":"StatementMeta(, 383bf70c-1243-43bf-a33e-fb9f96ddd4c5, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting openai\n  Downloading openai-1.52.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from openai) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from openai) (1.8.0)\nCollecting httpx<1,>=0.23.0 (from openai)\n  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\nCollecting jiter<1,>=0.4.0 (from openai)\n  Downloading jiter-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nCollecting pydantic<3,>=1.9.0 (from openai)\n  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sniffio in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from openai) (1.3.0)\nRequirement already satisfied: tqdm>4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from openai) (4.65.0)\nCollecting typing-extensions<5,>=4.11 (from openai)\n  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: idna>=2.8 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\nRequirement already satisfied: certifi in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\nCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\nCollecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\nCollecting pydantic-core==2.23.4 (from pydantic<3,>=1.9.0->openai)\n  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nDownloading openai-1.52.0-py3-none-any.whl (386 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiter-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m206.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: typing-extensions, jiter, h11, annotated-types, pydantic-core, httpcore, pydantic, httpx, openai\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.9.0\n    Not uninstalling typing-extensions at /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages, outside environment /nfs4/pyenv-5d70a06f-9923-46ef-a3de-ec814123db94\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\nSuccessfully installed annotated-types-0.7.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 openai-1.52.0 pydantic-2.9.2 pydantic-core-2.23.4 typing-extensions-4.12.2\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nWarning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"95786f67-4680-4da4-a583-a3e41e7052a2"},{"cell_type":"code","source":["import requests\n","from base64 import b64encode\n","import json\n","import os\n","from openai import AzureOpenAI\n","from pyspark.sql.functions import *\n","from notebookutils.mssparkutils.credentials import getSecret\n","import ast"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"383bf70c-1243-43bf-a33e-fb9f96ddd4c5","normalized_state":"finished","queued_time":"2024-10-20T13:34:51.6754633Z","session_start_time":null,"execution_start_time":"2024-10-20T13:35:22.6492685Z","execution_finish_time":"2024-10-20T13:35:23.4866167Z","parent_msg_id":"5edb51d0-e098-4e2b-a900-ac5e58e0542a"},"text/plain":"StatementMeta(, 383bf70c-1243-43bf-a33e-fb9f96ddd4c5, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d30f7260-60fe-43e0-9eec-3e371b95f911"},{"cell_type":"code","source":["%run \"/configuration\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[10,11],"state":"finished","livy_statement_state":"available","session_id":"383bf70c-1243-43bf-a33e-fb9f96ddd4c5","normalized_state":"finished","queued_time":"2024-10-20T13:34:51.6761682Z","session_start_time":null,"execution_start_time":"2024-10-20T13:35:25.5331409Z","execution_finish_time":"2024-10-20T13:35:25.5334124Z","parent_msg_id":"18d1c112-ec09-4ccf-96e5-a5e94bc0d6e0"},"text/plain":"StatementMeta(, 383bf70c-1243-43bf-a33e-fb9f96ddd4c5, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc1fdcf1-696a-481e-a712-52747d2816f9"},{"cell_type":"code","source":["keyvault_endpoint = \"https://hackathonkv.vault.azure.net/\"\n","\n","# Get PAT Key\n","pat_key = getSecret(keyvault_endpoint, \"azuredevoppat\")\n","\n","# Azure OpenAI\n","openai_endpoint = getSecret(keyvault_endpoint, \"opeaiendpoint\")\n","openai_key = getSecret(keyvault_endpoint, \"openaikey\")\n","openai_api_version = getSecret(keyvault_endpoint, \"openaiapiversion\")\n","openai_model = getSecret(keyvault_endpoint, \"openaimodelname\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"383bf70c-1243-43bf-a33e-fb9f96ddd4c5","normalized_state":"finished","queued_time":"2024-10-20T13:34:51.6769261Z","session_start_time":null,"execution_start_time":"2024-10-20T13:35:25.9966883Z","execution_finish_time":"2024-10-20T13:35:29.8560411Z","parent_msg_id":"a9f6d02f-ce64-4c58-9a76-a4ac748c84cc"},"text/plain":"StatementMeta(, 383bf70c-1243-43bf-a33e-fb9f96ddd4c5, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"367ac550-ed70-4fcb-a5e4-f73c453ffe83"},{"cell_type":"code","source":["class AzureDevOps_API:\n","    def __init__(self, organization, token):\n","        self.organization = organization\n","        self.token = token\n","\n","    def header_generate(self):\n","        authorization = str(b64encode(bytes(':'+ self.token, 'ascii')), 'ascii')\n","        headers = {'Accept': 'application/json','Authorization': 'Basic '+ authorization}\n","        return headers\n","    \n","    def get_wikipage_content(self, project, wikiIdentifier, includeContent=True, recursionLevel=\"OneLevel\",api_version=\"7.2-preview.1\"):\n","        if wikiIdentifier != None:\n","            url = f\"https://dev.azure.com/{self.organization}/{project}/_apis/wiki/wikis/{wikiIdentifier}/pages?recursionLevel={recursionLevel}&includeContent={includeContent}&api-version={api_version}\"\n","            headers = self.header_generate()\n","            response = requests.get(url, headers=headers)\n","\n","            # Check for HTTP errors \n","            if response.status_code != 200:\n","                print(f\"Error: {response.status_code} - {response.text}\")\n","                return None \n","            else:\n","                return response.json()\n","        else:\n","            return \"No Wiki documetation\"\n","\n","    def search_workitem_in_azure(self, search_text, projects=None, api_version = \"7.2-preview.1\", top_result = 0):\n","        url = f\"https://almsearch.dev.azure.com/{self.organization}/_apis/search/workitemsearchresults?api-version={api_version}\"\n","        headers = self.header_generate()\n","\n","        # Create the request body with the search keyword\n","        body = {\n","            \"searchText\": search_text,\n","            \"$top\": top_result,\n","            \"filters\": {\n","                \"System.TeamProject\": projects if projects else [],\n","            },\n","            \"includeFacets\": True,\n","            \"includeSnippet\": True\n","        }\n","\n","        # Send the POST request\n","        response = requests.post(url, headers=headers, json=body)\n","\n","        # Check the response status\n","        if response.status_code == 200:\n","            return response.json()  # Return the results as JSON\n","        else:\n","            print(f\"Error: {response.status_code} - {response.text}\")\n","            return None\n","    \n","    def export_json_to_file(self, response_data, folder_path, file_path, max_char = 5000):\n","\n","        if not os.path.exists(folder_path):\n","            os.makedirs(folder_path)\n","            # print(f\"{folder_path} created successfully.\")\n","        # else: \n","            # print(f\"{folder_path} already created.\")\n","\n","        # try:\n","            # Attempt to convert the response_data to JSON format if it's not already a dict\n","            #if isinstance(response_data, dict):\n","            #    json_data = response_data\n","            #else:\n","                # Try to parse the response_data into a JSON object\n","            #    json_data = json.loads(response_data)\n","            \n","        # Convert JSON data to string\n","        json_str = json.dumps(response_data, ensure_ascii=False, indent=4)\n","        # Check if the length exceeds max_characters\n","        # if len(json_str) > max_char:\n","        #    json_str = json_str[:max_char] # Truncate to max_character length\n","\n","        # Write the JSON data to the specified file path\n","        with open(file_path, 'w', encoding='utf-8') as json_file:\n","            json_file.write(json_str)\n","        # print(f\"JSON data has been exported successfully to {file_path}\")\n","\n","        # except json.JSONDecodeError:\n","        #     print(\"The response data is not in valid JSON format.\")\n","        # except Exception as e:\n","        #     print(f\"An error occurred while exporting to JSON: {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"383bf70c-1243-43bf-a33e-fb9f96ddd4c5","normalized_state":"finished","queued_time":"2024-10-20T13:34:51.6777208Z","session_start_time":null,"execution_start_time":"2024-10-20T13:35:30.3158007Z","execution_finish_time":"2024-10-20T13:35:30.5793231Z","parent_msg_id":"65aa4b0a-ae9f-491e-bead-169715df2fa6"},"text/plain":"StatementMeta(, 383bf70c-1243-43bf-a33e-fb9f96ddd4c5, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4668f7ea-387d-4560-98ff-a9659cc7187a"},{"cell_type":"code","source":["class AzOpenAI:\n","    def __init__(self, endpoint, api_key, api_version, model_name):\n","        self.endpoint = endpoint\n","        self.api_key = api_key\n","        self.api_version = api_version\n","        self.model_name = model_name\n","    \n","    def get_search_keyword(self, user_question):\n","\n","        client = AzureOpenAI(\n","            azure_endpoint = self.endpoint,\n","            api_key = self.api_key,\n","            api_version = self.api_version\n","        )\n","\n","        response = client.chat.completions.create(\n","            model = self.model_name,\n","            response_format = {\"type\": \"text\"},\n","            messages=[\n","                {\"role\": \"system\", \"content\": \"\"\"\n","                Define the most important topic keyword based on the user question below:\n","                If user write any phrases without spaces between the words, let treat them as one keyword. \n","                If there are more one topic keyword, provide them as a list data type. \n","                If there is just one topic keyword, provide it as a string\n","                \"\"\"},\n","                {\"role\": \"user\", \"content\": user_question}\n","            ]\n","        )\n","        result = response.choices[0].message.content\n","        return result\n","    \n","    def get_answers(self, json_content, keyword):\n","\n","        client = AzureOpenAI(\n","            azure_endpoint = self.endpoint,\n","            api_key = self.api_key,\n","            api_version = self.api_version\n","        )\n","\n","\n","        response = client.chat.completions.create(\n","            model = self.model_name,\n","            response_format = {\"type\": \"text\"},\n","            messages=[\n","                {\"role\": \"system\", \"content\": f\"\"\"You are an AI assistant that helps to find the information from text file, the keyword here is {keyword} and write the short comprehensive answer aboput that keyword as text.\"\"\"},\n","                {\"role\": \"user\", \"content\": json_content}\n","            ]\n","        )\n","        result = response.choices[0].message.content\n","        return result"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"383bf70c-1243-43bf-a33e-fb9f96ddd4c5","normalized_state":"finished","queued_time":"2024-10-20T13:34:51.6785297Z","session_start_time":null,"execution_start_time":"2024-10-20T13:35:31.0181916Z","execution_finish_time":"2024-10-20T13:35:31.2645266Z","parent_msg_id":"9074effb-af3f-45b4-ad44-cbe9fd7f313c"},"text/plain":"StatementMeta(, 383bf70c-1243-43bf-a33e-fb9f96ddd4c5, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"62dac5b0-6e9c-4ab6-a7dd-125e4fcf9577"},{"cell_type":"code","source":["def get_json(path):\n","    if 'lakehouse/default' in path:\n","        path = path.split(\"lakehouse/default/\")[1]\n","    json_dict = spark.read.option(\"multiline\", \"true\").json(path).select('facets')\n","    json_rows = json_dict.collect()\n","    # Convert rows to a list of dictionaries\n","    json_dict_list = [row.asDict() for row in json_rows]\n","    return json_dict_list"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"383bf70c-1243-43bf-a33e-fb9f96ddd4c5","normalized_state":"finished","queued_time":"2024-10-20T13:34:51.6793493Z","session_start_time":null,"execution_start_time":"2024-10-20T13:35:31.7572252Z","execution_finish_time":"2024-10-20T13:35:32.0134921Z","parent_msg_id":"4d828283-a884-48f3-8b40-e2ed0c046686"},"text/plain":"StatementMeta(, 383bf70c-1243-43bf-a33e-fb9f96ddd4c5, 15, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6e53f2b6-7551-420f-8d5c-4243e1dd955a"},{"cell_type":"code","source":["def project_iq(user_question):\n","    openai = AzOpenAI(openai_endpoint, openai_key, openai_api_version, openai_model)\n","    keyword_result = openai.get_search_keyword(user_question)\n","\n","    if \"[\" in keyword_result and \"]\" in keyword_result:\n","        keyword_result = ast.literal_eval(keyword_result)\n","        for keyword in keyword_result: \n","            devops = AzureDevOps_API(configuration_para['organization'], pat_key)\n","            search_response = devops.search_workitem_in_azure(keyword, configuration_para['projects'])\n","            devops.export_json_to_file(search_response, folder_path, folder_path + f\"{keyword.replace(' ', '')}\" + \".json\")\n","    else:\n","        devops_api = AzureDevOps_API(configuration_para['organization'], pat_key)\n","        search_response = devops_api.search_workitem_in_azure(keyword_result, configuration_para['projects'])\n","        devops_api.export_json_to_file(search_response, folder_path, folder_path + f\"{keyword_result.replace(' ', '')}\" + \".json\")\n","\n","    final_answer = \"\" \n","    if isinstance(keyword_result, list):\n","        for index, keyword in enumerate(keyword_result):\n","            json_file = get_json(folder_path + f\"{keyword.replace(' ', '')}\" + \".json\")\n","            openai = AzOpenAI(openai_endpoint, openai_key, openai_api_version, openai_model)\n","            answer = f\"{index+1}. \" + f\"{openai.get_answers(str(json_file), keyword)}\"\n","            final_answer += answer\n","    else:\n","        json_file = get_json(folder_path + f\"{keyword_result.replace(' ', '')}\" + \".json\")\n","        openai = AzOpenAI(openai_endpoint, openai_key, openai_api_version, openai_model)\n","        answer = openai.get_answers(str(json_file), keyword_result)\n","        final_answer = answer \n","    return print(final_answer)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","session_id":"383bf70c-1243-43bf-a33e-fb9f96ddd4c5","normalized_state":"finished","queued_time":"2024-10-20T13:52:42.5324673Z","session_start_time":null,"execution_start_time":"2024-10-20T13:52:43.0132009Z","execution_finish_time":"2024-10-20T13:52:43.2981226Z","parent_msg_id":"e3de5723-21e2-47c8-a2bb-4be7af6429fa"},"text/plain":"StatementMeta(, 383bf70c-1243-43bf-a33e-fb9f96ddd4c5, 22, Finished, Available, Finished)"},"metadata":{}}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0ea18aea-c576-4ac5-9d62-4c8b598c1dda"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ddd6f05a-ea49-4990-befd-6a0767908486"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"version":"0.1","state":{}},"dependencies":{"warehouse":{"known_warehouses":[{"id":"6bed1d41-9a49-4647-9375-d0fdb2df0179","type":"Lakewarehouse"}],"default_warehouse":"6bed1d41-9a49-4647-9375-d0fdb2df0179"},"lakehouse":{"default_lakehouse":"88f5ef03-f688-4cec-8e01-ec677280c74b","default_lakehouse_name":"Knowledge_Based","default_lakehouse_workspace_id":"0bcfd8e4-3d79-46e7-947a-06ccc4eebcea"}}},"nbformat":4,"nbformat_minor":5}